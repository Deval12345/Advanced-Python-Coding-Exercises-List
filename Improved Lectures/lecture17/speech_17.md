Welcome back.

Over the last several lectures, we have built a detailed map of Python's attribute system. We have seen descriptors intercept reads and writes. We have seen __getattr__ handle missing attributes. We have formalized behavioral contracts with ABCs. And through all of that, we focused on one thing: how Python finds attributes. Today, we ask a completely different question. How does Python store attributes? Where do they live in memory? What does that cost? And what does that cost look like when your program has not ten objects, but ten million?

This is the point in the course where Python development stops being purely about elegant design and starts being about hard engineering discipline.

Let me start with a fact that surprises many developers. Every Python object, by default, carries a dictionary inside it. Not a metaphorical dictionary. An actual Python dict object, allocated separately in memory, attached to every single instance you create. This dictionary is called __dict__, and it is the reason Python objects are so flexible. When you write self.name equals something inside __init__, Python is not writing to a fixed memory slot. It is inserting a key-value pair into that dictionary. And because it is a real dictionary, you can add new attributes to any object at runtime. Just write myObject.newAttribute equals "hello", and Python inserts the new key without complaint.

Now here is the cost. A Python dictionary on a 64-bit system costs at minimum 232 bytes just to exist. That is before you store a single attribute value. Every instance of every class pays this cost. If your class has ten million instances in a data processing pipeline, you are consuming 2.3 gigabytes of memory on dictionaries alone, before a single piece of actual data is stored. For classes that represent sensor readings, financial orders, or simulation particles, that overhead is unacceptable.

This is the problem that __slots__ was designed to solve.

When you declare __slots__ in a class body, you are telling the Python interpreter: do not create __dict__ for instances of this class. Instead, allocate a fixed array of storage cells when each instance is created — one cell per name you declare. This is how C structures work. A C struct is a block of memory with named fields at known, fixed offsets. Python's __slots__ mechanism gives you something equivalent at the Python level. When you write self.x inside a slotted class, Python does not perform a dictionary lookup. It computes the fixed offset for x within the object's layout and reads directly from that address. This is both faster and dramatically smaller.

Let me show you the actual numbers. We define two classes that do identical things: they each store three floating-point values named x, y, and z. The first class, PointWithDict, is a plain Python class. The second, PointWithSlots, adds a single line at the top of the class body: __slots__ equals a tuple of 'x', 'y', and 'z'. We create one instance of each with the same values and measure their sizes. For the dict-based point, we must add sys.getsizeof of the instance itself and sys.getsizeof of its __dict__, because the dictionary is a separate allocation. For the slotted point, there is no __dict__, so we measure only the instance. The typical result: the dict-based point uses around 232 to 280 bytes; the slotted point uses around 56 bytes. A ratio of roughly four to one for the same logical data.

The trade-off is explicit. With __slots__, you can no longer add arbitrary attributes at runtime. Try to write mySlottedObject.newField equals "hello", and Python raises AttributeError immediately. The set of attributes is fixed at class definition time. This is the engineering bargain: I know the shape of this object. I am committing to it. Give me the memory and speed in return.

This is why the libraries you depend on use __slots__ heavily. Pandas uses slotted internal structures. NumPy uses slotted descriptor objects. High-frequency trading systems use slotted order and quote objects created and destroyed millions of times per second.

Now, slots become more complex when inheritance is involved, and this is where most developers who know about slots still get it wrong. When you subclass a slotted class, the subclass inherits the parent's slots. The subclass can also declare its own __slots__, adding new named cells without duplicating the parent's. But here is the critical rule: if the subclass does not declare __slots__, Python automatically gives it a __dict__. Not instead of the parent's slots — in addition to them. The parent slots still exist, but the subclass adds a dictionary on top. Most of your memory savings evaporate.

The reason is that the default Python behavior is always to create __dict__. By omitting __slots__ in a subclass, you fall back to that default. The rule to keep in mind is absolute: every class in an inheritance chain that you want to benefit from slots must define __slots__. One missing class anywhere in the hierarchy brings __dict__ back for that branch.

Let me show you a correct multi-level slotted hierarchy. BaseRecord declares __slots__ with 'recordId' and 'timestamp'. SensorRecord inherits from BaseRecord and declares its own __slots__ with 'sensorId', 'value', and 'unit'. A SensorRecord instance ends up with all five slots: two from the parent and three of its own. We check hasattr of the record for __dict__, and it comes back False. There is no dictionary anywhere in this object. It is a tight, fixed-layout structure, and the interface to the outside world looks exactly like a regular class.

So when should you use slots, and when should you leave them out? Use __slots__ when you have thousands to millions of instances, when the attributes are known at design time, and when memory or access speed is a real constraint you have measured. The canonical cases are records in a data pipeline, nodes in a large tree, particles in a simulation, orders in a trading system.

Do not use __slots__ when you need dynamic attributes at runtime. Configuration objects that pick up arbitrary settings, test mocks that set arbitrary attributes on the fly, ORM objects that some serialization libraries manipulate through __dict__ directly — none of these are good candidates. Also, if your class is instantiated only ten times in the entire program, the savings are too small to matter. Do not add complexity for no measurable gain.

There is also a useful middle ground. If you include the string '__dict__' inside your __slots__ tuple, Python gives you both: named slots for the attributes you declared, and a __dict__ for anything dynamic. The named slots still get fixed-offset access; the __dict__ handles the rest. This is the escape hatch when you need performance for some attributes and flexibility for others.

Measuring memory correctly requires one more tool. sys.getsizeof tells you the size of one object's outer shell. The strings it references, the other objects it holds — those are all separate allocations that getsizeof does not follow. The right tool for total allocation measurement is tracemalloc. This is part of the standard library. You call tracemalloc.start, run your code, call tracemalloc.take_snapshot, and analyze it. In industry, this is how teams catch memory regressions. A pipeline that used to run in 4 GB now needs 6 GB. Tracemalloc tells you which line caused the spike.

When we compare 100,000 dict-based records against 100,000 slotted records using tracemalloc, the total allocation difference is always dramatic. The exact numbers depend on your Python version and platform, but the savings from slots are consistently large and consistently measurable.

Let me close with the core picture. Python objects use __dict__ by default: flexible, dynamic, and expensive. __slots__ replaces __dict__ with a fixed array of named cells: fast, compact, and committed to a known layout. Every class in an inheritance hierarchy must define __slots__ to get the full benefit. Use slots for classes with many instances where attributes are known at design time. Measure with tracemalloc to confirm the gain.

In the next lecture, we open the concurrency chapter. We stop asking how individual objects are built and start asking how programs handle multiple things happening at once. The first question in any concurrency conversation is always: what kind of work are you doing? That question determines everything else.
