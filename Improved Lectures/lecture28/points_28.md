# Key Points — Lecture 28: Sharing NumPy Data with Multiprocessing — Zero-Copy Design

---

- **Sending a NumPy array through a multiprocessing Queue causes three full data copies and can consume seconds of serialization time for large arrays.** When Queue.put(array) is called, pickle serializes the array — dtype, shape, strides, and every byte of data — into a bytes buffer in the sending process (copy one). Those bytes are written through a kernel pipe (copy two). The receiving process reconstructs the full array from bytes (copy three). A 100-megabyte array generates 300 megabytes of peak memory pressure during transit. At 30 arrays per second, the pipeline requires 1.8 gigabytes per second of pickling throughput — unsustainable for any Python process. This makes Queue unsuitable as a data channel for large numerical arrays.

- **The zero-copy insight is that a NumPy array is bytes plus metadata — if the bytes live in shared memory, each process wraps them in a lightweight NumPy view without any copying.** A NumPy array object is a small Python structure containing a pointer to a data buffer, dtype, shape, and strides. If multiple processes map the same physical memory pages into their address spaces, each process can create its own NumPy array object pointing into the shared bytes. The array object is kilobytes; the data buffer is megabytes — and it never moves. numpy.frombuffer takes any object implementing the Python buffer protocol and wraps its bytes as a NumPy array, with zero allocation.

- **multiprocessing.Array creates a typed shared memory buffer compatible with the buffer protocol, enabling direct frombuffer wrapping in any participating process.** Creating a multiprocessing.Array with ctypes.c_double allocates a contiguous block of double-precision floating-point bytes in a memory-mapped region shared by all processes that receive the Array reference. numpy.frombuffer wraps those bytes as a float64 array with no data movement. Workers receive the Array reference — not the data — and create their own frombuffer views pointing to the same physical bytes. Writes from any worker are immediately visible to all other processes because they all share the same physical memory.

- **multiprocessing.shared_memory, introduced in Python 3.8, provides a cleaner alternative with raw-bytes allocation, name-based access, and full NumPy dtype support.** The main process allocates a named block of raw bytes sized to the array's byte count. A NumPy ndarray is constructed over the block's buffer using the dtype and shape as arguments — no ctypes type codes required, supporting structured dtypes and multi-dimensional layouts. Workers receive the name string — a tiny IPC payload — open the block by name independently, and create their own NumPy views. The name-based access mechanism works across unrelated processes, enabling server architectures where a data-loading process pre-populates shared memory and multiple independent worker processes attach by name.

- **The shared_memory lifecycle requires careful management: close releases the process's mapping; unlink destroys the OS-level resource.** Calling sharedMem.close() in a worker releases that process's virtual memory mapping but does not free the underlying shared memory. Calling sharedMem.unlink() in the creator process destroys the OS resource, freeing the physical memory. Forgetting to unlink leaves an orphaned shared memory block in the OS registry that persists until system reboot. The correct pattern is to wrap the creator's lifecycle in a try-finally block: close and unlink in the finally clause, ensuring cleanup even if worker processes raise exceptions.

- **The lock discipline for shared memory reduces to three rules: concurrent reads are always safe; partitioned writes to non-overlapping regions are always safe; overlapping writes require explicit locks.** When all workers read from the array without writing, no lock is needed — reads of the same memory address from multiple processes are safe at the hardware level. When workers write to non-overlapping index ranges — worker zero writes indices 0 to N/4, worker one writes N/4 to N/2 — they operate on different physical memory addresses and cannot conflict. Locks are required only when two or more workers write to the same address, which the standard partitioned design eliminates entirely by construction.

- **The partitioned-write pattern enables parallel in-place transforms without any locks by assigning non-overlapping array slices to each worker.** The main process divides the array into contiguous chunks based on worker count, computes start and end indices for each, and passes those indices to workers as arguments. Workers compute their chunk independently, writing results directly into shared memory. After all workers join, the main process's view reflects all changes immediately, with no result transfer. This is MapReduce at the shared-memory level: the map phase is the parallel in-place transform; the reduce phase is optional sequential aggregation in the main process.

- **The correct IPC design for large-array pipelines sends names, indices, and statistics — never the arrays themselves.** In a well-designed shared-memory pipeline, the IPC payloads are: a name string (dozens of bytes) when workers attach to shared memory, integer chunk boundaries (a few bytes) when workers know their work regions, and small result structures like statistics dictionaries (hundreds of bytes) when workers return computed values. The arrays themselves never enter the IPC channel. This pattern reduces IPC bandwidth from megabytes-per-task to kilobytes-per-task, making the coordination overhead negligible relative to the computation time on any non-trivial workload.
