# Lecture 24: High-Level Concurrency APIs — Controlling Async at Scale
## speech_24.md — Instructor Spoken Narrative

---

In the last lecture we explored Futures and Executors — the mechanical stack of coroutines, the event loop, and the bridge to thread and process pools. You now understand how things work at the low level. Today we take one step up the abstraction ladder. Writing async code that works is one skill. Writing async code that is safe, bounded, and reliable under real production scale is a different skill entirely.

Let me paint a picture. You have an async service that fans out to an external API — say, fetching product information for an e-commerce catalog. You write what seems like a perfectly reasonable piece of code: gather with a list comprehension that spawns thousands of coroutines at once. You run it in testing with a hundred items. It works fine. You deploy. Your catalog has a hundred thousand products. Every one of those coroutines opens a network connection simultaneously. The target API receives a hundred thousand connection requests in the span of a millisecond. It responds with HTTP 429 — Too Many Requests. Your own process exhausts its file descriptor limit and crashes with "Too many open files." The code that worked perfectly in testing has become, in production, an accidental denial-of-service attack against your own vendor.

This is not a contrived example. This happens. And the five tools we study today are precisely the tools that prevent it from happening.

The first tool is asyncio.Semaphore. Think of a parking lot with N spaces. Each coroutine arriving at the semaphore is a car looking for a parking space. If there is a free space, it enters immediately. If all spaces are taken, it waits at the entrance. As a car leaves — as a coroutine finishes — it releases its space and the next waiting car can enter. The parking lot never overflows. It simply regulates the rate of entry. In code, this looks like wrapping your API call with async with semaphore. The semaphore enforces that at most N coroutines are in that section simultaneously — regardless of how many are waiting outside.

Let us look at Example 24.1 to see this in action. We set a maximum of three concurrent sessions and create the semaphore. Each coroutine enters the semaphore using the async with statement — a blocking operation if the semaphore is already at capacity. Once inside, it prints a timestamped start message, simulates a network call with asyncio.sleep, prints completion, and returns its result. In main we build nine coroutines and pass them all to gather at once. Without the semaphore, all nine would run simultaneously. With it, only three can be inside at any moment. The output timestamps confirm the batching: three sessions start at roughly zero seconds, three more at roughly 0.3 seconds, three more at 0.6 seconds. Total time is approximately 0.9 seconds — three sequential batches — rather than the 0.3 seconds of true unbounded concurrency. But without the collateral damage.

Now let us talk about asyncio.wait. You have used asyncio.gather — it waits for every task to finish before returning. If one task raises, gather cancels the rest by default. You cannot inspect results as tasks complete, and you cannot act on the first success without waiting for everyone else. For many real coordination patterns, gather is too blunt an instrument.

asyncio.wait was designed to return control to you as soon as a meaningful event occurs. The return when parameter encodes three coordination strategies. FIRST COMPLETED — the race pattern: give it a set of tasks and it returns the moment any one of them finishes. FIRST EXCEPTION — the fail-fast pattern: return the moment any task raises. ALL COMPLETED — same as gather but with finer control: you can inspect each task's result and exception separately rather than having gather re-raise everything at once.

The race pattern is ubiquitous in distributed systems. A content delivery network submits the same request to multiple regional endpoints simultaneously and uses whichever responds first. A database client submits reads to a primary and a replica simultaneously and uses the first response. With asyncio.wait and FIRST COMPLETED, the winning result is available immediately and the losing tasks are cancelled cleanly.

Let us trace through Example 24.2. We create three tasks, each querying a different geographic region — us-east with a 0.2 second base delay, eu-west at 0.3 seconds, ap-south at 0.4 seconds — each with random jitter so the winner varies between runs. We pass all three to asyncio.wait with return when set to FIRST COMPLETED. The moment the first one finishes, wait returns immediately. We pop the winning task from the done set and print its result. We then cancel every task in the pending set and await them through gather with return exceptions set to True. That final gather is critical — it gives the event loop the opportunity to deliver the cancellation and let the cancelled tasks clean up before we exit. Without it, we leave dangling tasks that the event loop warns about on shutdown.

Next: asyncio.wait for and timeouts. A coroutine waiting on a network call can wait forever if the target server is down or overloaded. Without an explicit timeout, that coroutine holds its slot in the event loop indefinitely. In a server handling thousands of concurrent requests, a single hung coroutine is a minor annoyance; a flood of them — all waiting on a broken external dependency — is a cascading failure that takes the entire service down.

asyncio.wait for wraps any coroutine and raises asyncio.TimeoutError after a specified number of seconds if the coroutine has not returned. The subtlety that trips people up: when TimeoutError fires, the underlying coroutine is not simply abandoned. It is cancelled at its next await point. This means the cancelled coroutine's cleanup code still runs — it can close sockets, log the timeout, roll back partial writes. Production retry patterns combine wait for with an exponential backoff loop: attempt the call, catch TimeoutError, sleep briefly, retry — up to a maximum number of attempts. This is the foundation of resilient service clients.

In Example 24.3, the unreliable API coroutine simulates a service with variable response times — anywhere from 0.1 to 2.0 seconds. The callWithTimeout coroutine wraps it with asyncio.wait for and a 0.5-second deadline. If the call returns in time, we print success and return. If not, wait for raises TimeoutError, we catch it, print the timeout, and the loop continues to the next attempt. In main we run three independent callers concurrently. Each has its own retry state; a timeout in caller A does not affect caller B. The output shows a mixture of successes on varying attempt numbers and failures when all retries are exhausted — a realistic simulation of a production service with unreliable upstream dependencies.

Now asyncio.Queue. asyncio.gather requires you to know all your tasks upfront. But many real workloads are dynamic: a data pipeline where a producer generates work items continuously from an external feed; a web scraper where the set of URLs grows as each page is parsed; a message processor where work arrives from a queue service indefinitely. For these patterns, the producer-consumer architecture is the right model.

asyncio.Queue is the async-native equivalent of threading queue.Queue. It provides awaitable put and get methods. Its maxsize parameter enforces a backpressure limit — when the consumer is slower than the producer, the queue fills, and each subsequent await queue.put blocks the producer until space is available, automatically throttling the source rate. maxsize is not an optional parameter in production systems; it is the safety valve that prevents a fast producer from exhausting memory.

Example 24.4 shows the producer-consumer pattern in action. We create a queue with a maximum size of five. The producer generates ten work items and awaits queue.put for each one. After all items are produced, it puts a None sentinel to signal shutdown. Each consumer loops on await queue.get. When it receives None, it re-puts the sentinel so the other consumer also receives the shutdown signal, then breaks. That re-put sentinel pattern ensures a single None correctly shuts down all consumers regardless of how many there are.

Finally: asyncio.Event. Sometimes coordination is not about limiting access or passing data — it is about timing. One coroutine needs to wait until another signals that some condition is true. A connection pool needs to pause all callers while it initializes, then release them all at once when initialization completes.

asyncio.Event models a one-shot binary signal. It starts unset. Any number of coroutines can await event.wait — they all pause there. When one coroutine calls event.set, every waiting coroutine is immediately scheduled to resume. The event remains set; any future coroutine that calls wait on an already-set event continues immediately. This is edge-triggered: the wakeup happens the instant the flag is set, with no polling delay and no wasted work between checks.

These five tools are the composition layer of async Python. They are not alternatives to gather and create task — they are the structures you build on top of them when your system grows beyond a simple fan-out. Semaphore encodes resource ceilings. Wait gives fine-grained task coordination. Wait for enforces deadlines. Queue implements streaming producer-consumer. Event provides one-shot coordination signals. Together they are what separate an async script from a production async system.

