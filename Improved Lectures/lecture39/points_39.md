# Key Points — Lecture 39: Big Project Stage 6 — Resilience Layer

---

- **Retry with exponential backoff is the correct strategy for transient failures — failures that self-resolve within seconds — because it gives the failing system time to recover while distributing client retries across time to prevent thundering herd effects.** Exponential backoff doubles the wait time after each failure: `delay = base × 2^attempt`. Adding jitter — `delay += random.uniform(0, 1)` — ensures that multiple clients retrying simultaneously do not all wait the same duration and retry simultaneously on the next attempt. A maximum delay (`min(delay, maxDelay)`) prevents unbounded waits after many consecutive failures. The retry decorator wraps the function transparently using `@functools.wraps`, preserving the function's identity in stack traces and documentation. Exceptions in the `exceptions` tuple are retried; all other exceptions propagate immediately — never retry programming errors like `TypeError` or `ValueError`.

- **The circuit breaker pattern complements retry logic by handling sustained failures — failures that last minutes or hours — by failing fast after a threshold is crossed, preserving the caller's resources and giving the failing service time to recover without load.** The three-state machine: "closed" (normal operation — failures accumulate against the threshold), "open" (fast-fail — calls raise `CircuitOpenError` in microseconds without I/O), "half-open" (probing — one test call is allowed through; if it succeeds, the circuit closes; if it fails, it reopens). Without a circuit breaker, retrying against a service that is down for 30 minutes consumes 30 minutes of accumulated retry waits across all callers. With a circuit breaker, after the threshold is crossed, all calls fail in < 1ms for the duration of the outage. `time.monotonic()` is used for the reset timeout — it is immune to system clock adjustments that could corrupt the elapsed time calculation.

- **Graceful degradation converts partial failures into bounded output reduction rather than total output loss, by capturing failure at the source component and returning structured sentinel values that downstream stages can consume or skip.** For a five-sensor pipeline where one sensor fails, the naive behavior is complete pipeline failure — no readings from any sensor are processed. The graceful degradation pattern wraps each sensor read in a try/except that returns a sentinel (`{"status": "unavailable", "value": None}`) on failure rather than propagating the exception. The batch collector separates successful reads from failed ones; downstream stages receive only successful reads and operate normally. Batch metadata (`"degraded": True, "unavailableSensors": ["S3"]`) provides the information needed for monitoring and root-cause analysis. The pipeline continues producing value at reduced capacity — 80% output from a five-sensor pipeline with one failed sensor is far better than 0% output from a crashed pipeline.

- **The three resilience patterns — retry, circuit breaker, and graceful degradation — are complementary and should be stacked for comprehensive failure handling.** Retry handles the first few transient failures (1–3 retries over seconds). If all retries fail, the circuit breaker accumulates the failure against its threshold; when the threshold is crossed, the circuit opens and subsequent calls fail fast. At the batch collection level, graceful degradation catches failures (whether from exhausted retries or circuit-open errors) and converts them to sentinel values, allowing the pipeline to continue. The stack is: `readSensor` → `@retryWithBackoff` → `AsyncCircuitBreaker` → `readSensorWithFallback`. A transient failure triggers retry and succeeds. A sustained failure exhausts retries, opens the circuit, and subsequent calls fast-fail into the fallback. The pipeline always continues.

- **The design principle of Stage 6 is that failure is a first-class concern, not an edge case.** In a pipeline running continuously with many external dependencies, some dependency will fail at some point — this is not exceptional, it is normal. A system with no resilience layer has undefined behavior on failure: it may crash silently, propagate corrupt state, or hang indefinitely waiting for a response that will never come. A system with explicit resilience patterns has defined behavior: retry for transient failures, fast-fail for sustained failures, partial output for partial failures. The distinction between undefined and defined failure behavior is the difference between systems that require constant human intervention during incidents and systems that handle incidents automatically, alerting operators to act only when human judgment is genuinely required.

