# Key Points — Lecture 30: Race Conditions and Deadlock in Concurrent Python

---

- **A race condition occurs when two concurrent threads execute a read-modify-write sequence on shared state without coordination, and the final value depends on which thread writes last rather than on the intended logic.** Incrementing a counter is not one operation — it is three: read the current value, compute the new value, write the new value back. Between the read and the write, another thread can execute its own read of the same value. Both threads compute their update from the same stale value and write back, and one update is silently lost. This is the lost-update pattern. With ten threads each incrementing a counter 100 times, the expected result is 1,000. A race condition may produce 720 or 850 — consistently wrong, no error raised, no exception thrown.

- **Race conditions are non-deterministic and load-sensitive, making them extremely difficult to detect through standard testing.** The race window — the gap between a thread's read and write — may be nanoseconds. Under light load with one or two threads, the probability of two threads occupying the same window simultaneously is negligible. Under production load with many threads executing at high frequency, the probability approaches certainty. A system that passes all tests in a development environment with synthetic load may exhibit race conditions for the first time only under real production traffic, producing the "works on my machine" failure pattern. The fix must be structural — not based on the observation that the race has not appeared yet.

- **threading.Lock is the fundamental solution to race conditions: it makes the critical section — the read-modify-write sequence — atomic from the perspective of all other threads.** A threading.Lock is a mutual exclusion primitive: only one thread can hold it at any moment. Any other thread that tries to acquire a held lock blocks until the lock is released. By wrapping the entire read-modify-write sequence inside `with self._lock:`, you guarantee that no other thread can enter the same block while one is executing it. The block is atomic in the sense that it cannot be interleaved with another thread's execution of the same block.

- **The `with` statement for lock management is not a style preference — it is required for exception safety.** The pattern `lock.acquire(); critical_operation(); lock.release()` fails to release the lock when critical_operation raises an exception. The lock is permanently held; every subsequent thread that tries to acquire it blocks indefinitely. This is a lock leak, and it produces a deadlock-like freeze that is extremely difficult to diagnose because there is no deadlock — just a missing release. The context manager `with lock:` guarantees release at the end of the block on all exit paths, including exceptions. Always use the context manager.

- **Deadlock occurs when two or more threads form a circular dependency — each holds a resource the other needs, and none can proceed until the other releases.** Thread A holds lock L1 and is blocked waiting for lock L2. Thread B holds lock L2 and is blocked waiting for lock L1. Both are suspended. Neither can release its held lock because it is blocked before the release code. The system is frozen in these threads forever. Deadlock is silent: no exception, no error message, no crash — threads simply stop making progress. In a web server, deadlocked threads never return to the pool; new requests are accepted but accumulate until the thread pool is exhausted.

- **Deadlock prevention through consistent lock ordering eliminates the circular wait condition, making deadlock structurally impossible.** If every thread in the system that acquires both lock A and lock B always acquires A before B, then no thread can hold B and be waiting for A, because acquiring B requires having already acquired and released A. No circular dependency can form. The rule is: define a global lock hierarchy, document it explicitly, and enforce it in code review. For production systems with many locks, automated lock-order validators — such as the Linux kernel's lockdep subsystem — enforce the hierarchy at runtime.

- **threading.RLock (reentrant lock) solves the self-deadlock problem when a thread must acquire a lock it already holds.** A regular threading.Lock deadlocks a thread with itself: if the thread calls lock.acquire() while already holding the lock, the acquire blocks waiting for the lock to be released, but the release code is never reached because the thread is blocked. threading.RLock tracks the identity of the holding thread; the same thread can call acquire() N times without blocking, and the lock is only fully released when release() has been called N times. This is needed for recursive algorithms and class hierarchies where public methods call other public methods that protect themselves with the same lock.

- **threading.Condition provides atomic wait-and-notify semantics for producer-consumer coordination, eliminating busy-waiting on shared state.** A Condition wraps a lock. A waiting thread calls condition.wait() inside a while loop that checks the relevant state condition. The wait() call atomically releases the lock and suspends the thread, allowing the lock to be acquired by a producer. When the producer changes the state and calls condition.notify() or condition.notify_all(), the waiting thread wakes, re-acquires the lock, and rechecks the condition in the while loop. The while loop rather than an if statement is required because spurious wakeups can occur — the thread may wake without the condition being satisfied. This is the internal mechanism of Python's queue.Queue, used in every thread-safe producer-consumer pipeline.
