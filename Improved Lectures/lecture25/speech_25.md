# Lecture 25: Multiprocessing — True Parallelism Beyond the GIL
## speech_25.md — Instructor Spoken Narrative

---

Let me start with a concrete scenario. You are a media engineer at a company that processes user-uploaded photos. Every photo needs to be resized, color-corrected, and compressed before being served from a CDN. You have a batch of ten thousand photos to process overnight. You have a powerful machine with eight CPU cores. You write a Python script that processes photos one at a time. You run it. Seven of your eight CPU cores sit at zero percent utilization. The script takes eight hours. You add threads to parallelize. You now have eight threads, eight cores — but the Python GIL ensures only one thread executes Python bytecode at any moment. The other seven are waiting. Still eight hours.

This is the wall that the GIL builds. And multiprocessing is the door through it.

The key insight is the one we established in the GIL lecture: the GIL is per-interpreter. One Python interpreter, one GIL. But if you create eight separate Python interpreter processes — eight entirely independent programs with eight separate GILs — the OS can schedule each on its own core. They run simultaneously. Real parallel computation. Your eight-hour photo job becomes a two-hour photo job.

This is exactly what Python's multiprocessing module does. It creates separate OS processes, each with its own Python interpreter, its own GIL, its own memory space. No shared memory, no contention. When we say isolation by design, we mean it architecturally: processes cannot accidentally interfere with each other's data because their memory spaces are completely separate. That isolation is a safety guarantee, not just an implementation detail.

Now the first thing you need to understand is process start methods. This is platform-specific and it matters a great deal. On Linux, the default is fork: the child process gets an exact copy of the parent's memory through a mechanism called copy-on-write — the parent's memory is shared read-only until either process modifies it, at which point a private copy is made. Fork is fast — startup time is minimal. But it duplicates everything: open file descriptors, network sockets, database connections, locks. If any of those are in an inconsistent state at the moment of fork, the child inherits that inconsistency. On macOS since Python 3.8 and on Windows, the default is spawn: a fresh Python interpreter starts, imports the target module from scratch, and calls the target function. Spawn is safer but costs 50 to 200 milliseconds per process to start.

That spawn behavior explains one of the most important and non-obvious rules in multiprocessing: the if-name-equals-main guard. When Python spawns a new process, it imports your script. If your script contains code at the top level that creates more processes, each spawned process will also import the script, which spawns more processes, which import the script — an infinite fork bomb that consumes all system resources and crashes the machine. The guard prevents this: top-level code inside if name equals main only runs in the original process, not in spawned children. This guard is not a stylistic choice. It is a safety requirement.

Process objects are also heavy. We need to be honest about this. Spawning a process costs 50 to 200 milliseconds and uses 30 to 100 megabytes of memory, depending on the size of the loaded Python environment and imported libraries. Creating one process per tiny task defeats the purpose — the overhead dominates the computation. Process pools amortize this startup cost by keeping a fixed number of worker processes alive across many tasks. We will get to pools shortly.

Let us look at Example 25.1 to see raw process management in action. Four worker processes each do two million multiplications — real CPU computation — and write their results into a shared Manager list. The Manager creates a dedicated server process that hosts the shared list; all four workers communicate with it through inter-process communication. We record elapsed time before and after. On a multi-core machine, all four computations genuinely overlap in time.

Now we must talk about the fundamental constraint of multiprocessing: inter-process communication. Processes cannot share memory directly. If one process computes a result, it cannot simply write to a variable that another process reads. Those two variables live in completely separate memory spaces. Every piece of data crossing a process boundary must be serialized — pickled in Python's terminology — transmitted through a kernel-mediated channel, and deserialized on the other side.

Python provides several IPC primitives. multiprocessing.Queue is the general-purpose producer-consumer channel: process-safe, backed by a pipe and internal locks, suitable for distributing tasks and collecting results. multiprocessing.Pipe provides a direct bidirectional connection between exactly two endpoints, with lower overhead than Queue for point-to-point communication. multiprocessing.Value and multiprocessing.Array provide shared memory for small numeric data — counters, flags, progress indicators — without full serialization overhead. And multiprocessing.Manager, which we saw in Example 25.1, creates a server process that hosts shared Python objects like lists and dictionaries, flexible but the slowest option.

The pickling cost is the silent killer of multiprocessing performance. Sending a hundred megabyte NumPy array through a Queue means: pickle it into bytes, write those bytes through a pipe, unpickle it. That is three full copies of your data. A workload that sends large data to workers and receives large results can spend more time in IPC than in actual computation. The golden rule: minimize what crosses process boundaries. Send small task parameters to workers. Receive small results. Do all heavy data manipulation inside the worker, entirely within its own private memory.

Let us see this principle in Example 25.2. We use a producer-consumer pattern with a Queue. The main process puts prime-counting tasks onto a task queue — just integers, a few bytes each. Four workers each loop, pulling tasks, counting primes, and putting results back onto a result queue as tuples. When there are no more tasks, we send a None sentinel for each worker. Notice how small the inter-process messages are: integers going in, tuples of two integers coming out. The heavy computation happens entirely inside each worker's private memory.

Now multiprocessing.Pool. Manually managing Process objects, Queues, sentinels, and the producer-consumer protocol is verbose and error-prone. The pattern is so universal that Python provides it out of the box. Pool creates N worker processes at startup and keeps them alive for the lifetime of the pool. It amortizes startup cost across many tasks.

The API is deliberately modeled after concurrent.futures: pool.map distributes items across workers and collects results in submission order — blocking until all results arrive. pool.imap is the lazy version, yielding results as they complete, saving memory when the input is a large sequence. pool.apply async submits a single task asynchronously and returns an AsyncResult object. Used as a context manager with the with statement, Pool shuts down workers cleanly when the block exits. The chunksize argument for pool.map groups items into batches sent to each worker in one IPC message, dramatically reducing queue overhead when processing many small tasks.

Example 25.3 shows this concretely with an image resizing simulation. Eight images, processed first sequentially, then with a Pool of four workers. We time both, compute the speedup, and print a sample result to verify correctness. The speedup you see reflects real CPU parallelism: all four cores working simultaneously, each handling two images.

Here is the complete picture. Multiprocessing bypasses the GIL by giving each Python interpreter its own. You get true CPU parallelism across all available cores. The price is isolation: no shared memory, all communication through serialized IPC channels. Keep that communication minimal: small task parameters in, small results out, heavy computation entirely inside the worker. Process creation is heavyweight — use pools to amortize the cost. The if-name-equals-main guard is mandatory. Always join your processes; un-joined processes become zombies that hold system resources until the OS cleans them up.

In our next lecture, we go deeper: ProcessPoolExecutor as the high-level alternative to Pool, a frank analysis of IPC cost and when shared memory wins, and the scenarios where multiprocessing actually hurts — when process overhead dominates computation time and sequential code is faster.

