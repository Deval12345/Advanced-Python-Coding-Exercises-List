# Key Points — Lecture 25: Multiprocessing — True Parallelism Beyond the GIL

---

- **Multiprocessing achieves true CPU parallelism by giving each Python worker its own interpreter, its own GIL, and its own memory space.** The GIL is per-interpreter — one Python interpreter cannot run bytecode on more than one CPU core at a time, regardless of how many threads it has. By creating separate OS processes, multiprocessing sidesteps this constraint entirely. Each process runs its own GIL independently, and the OS scheduler runs them simultaneously on separate cores. A four-core machine running a Pool of four workers can deliver close to 4× the throughput of single-threaded code for CPU-bound tasks.

- **Process isolation is both a safety guarantee and a cost structure.** Separate memory spaces mean processes cannot accidentally corrupt each other's state — a bug in one worker process cannot corrupt data in another. This isolation is architecturally stronger than thread-based concurrency where a single misbehaving thread can corrupt shared state for all threads. The cost of that isolation is that every data transfer between processes must cross an explicit IPC boundary with serialization overhead.

- **The `if __name__ == "__main__"` guard is not optional — it is a safety requirement for all multiprocessing code on macOS and Windows.** When Python spawns a new process on these platforms, it imports the main script in the child process. Without the guard, the top-level code executes in the child — which includes the code that spawns more children — which import the script — which spawn more children. This creates an infinite process tree that consumes all system memory and CPU, crashing the machine. The guard ensures that process-creation code runs only in the original parent process.

- **Process startup is expensive: 50 to 200 milliseconds and 30 to 100 megabytes per process.** These costs make per-task process creation unacceptably expensive for short workloads. If each task takes 5 milliseconds and process startup takes 100 milliseconds, you spend 95% of your time on overhead. Process pools amortize this cost by creating N workers once at startup and reusing them across thousands of tasks. Use multiprocessing only when each task performs substantial CPU work — roughly 100 milliseconds or more — so the startup and IPC costs are negligible compared to the computation.

- **`multiprocessing.Queue` is the general-purpose IPC channel for the producer-consumer pattern across processes.** It is process-safe (internally uses a pipe and locks), supports any picklable Python object, and provides blocking `get()` and `put()` semantics. The sentinel pattern — putting `None` for each worker after all tasks are submitted — provides clean worker shutdown without requiring shared state or coordination outside the queue. Each worker receiving `None` exits its loop cleanly.

- **All data crossing a process boundary is pickled (serialized) and then unpickled (deserialized).** This is the silent killer of multiprocessing performance. Sending a 100 MB NumPy array through a Queue means serializing it into bytes, writing those bytes through a pipe, and deserializing them on the other side — three full data copies. A poorly designed workload can spend more time moving data between processes than actually computing. The golden rule: send small task parameters to workers, receive small results back, and perform all heavy data manipulation entirely inside the worker in its own private memory space.

- **`multiprocessing.Pool` is the high-level API for the common pattern of distributing independent tasks across a fixed worker pool.** `pool.map(fn, iterable)` distributes items, runs them in parallel across workers, and collects results in submission order. `pool.imap(fn, iterable)` is the lazy version that yields results as they complete, useful when the input is very large and you want to start processing results before all tasks finish. `pool.apply_async(fn, args)` submits a single task and returns an AsyncResult for non-blocking access to the result later.

- **The `chunksize` parameter for `pool.map` controls IPC granularity and is critical for performance with small tasks.** By default, pool.map sends one item per IPC message. With ten thousand small tasks, this means ten thousand separate IPC round-trips — each with pickling, pipe write, and pipe read overhead. Setting `chunksize=100` batches 100 items per message, reducing round-trips to one hundred and dramatically improving throughput. Higher chunksize helps when tasks are small and uniform; lower chunksize helps when tasks have highly variable durations and you want load balancing.

- **The correct data design for multiprocessing sends paths and indices, not payloads.** Production video transcoding pipelines do not send raw video frames through queues — they send file paths; each worker reads its own file from disk. Machine learning preprocessing systems do not send full datasets to workers — they send row index ranges; each worker reads its own slice from a memory-mapped file. This design keeps inter-process messages small regardless of input data size, ensuring that IPC overhead scales with the number of tasks rather than the size of the data being processed.
