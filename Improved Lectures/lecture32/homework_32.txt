Homework — Lecture 32: Importance of Speed in Real Systems

===============================================================
EXERCISE 1 — Data Structure Selection Benchmark
===============================================================

Build a systematic benchmark comparing performance of different data structures
for the same logical operation, and identify the crossover point where the better
structure becomes worth the overhead.

Requirements:

1. Write a function `measureMembershipTest(containerType, size, numLookups)` that:
   - Creates a container of the given type ("list", "set", "dict", "tuple") with
     integers 0 through size-1
   - Uses timeit to measure the cost of `target in container` for `numLookups` random targets
   - Returns the average time per lookup in microseconds

2. Run the benchmark for sizes [100, 1000, 10000, 100000] and all four container types.
   Print a table showing average lookup time (µs) for each size/type combination.
   Observe: list and tuple are O(N); set and dict are O(1).

3. Find the "crossover point" for list vs set: at what size does set become more than 10×
   faster than list for membership testing? (At very small sizes, list may be competitive
   due to Python overhead of hashing.)

4. Extend the benchmark to compare `collections.deque.appendleft(x)` vs `list.insert(0, x)`
   for N=100, 1000, 10000, 100000 elements. Measure 1000 front-insertions on each.
   This demonstrates O(1) vs O(N) insert at front.

5. Summarize: write a short comment (5–10 lines) explaining when to choose each data
   structure, based on your benchmark results. Include specific size thresholds you observed.

No solutions provided.

===============================================================
EXERCISE 2 — The Measure-Optimize-Remeasure Cycle
===============================================================

Apply the full profiling discipline to identify and fix a real bottleneck in a
provided pipeline, then verify the improvement.

Requirements:

1. Implement this "slow" pipeline exactly:

   def slowDeduplicator(records):
       seen = []          # using LIST for seen tracking
       out = []
       for r in records:
           if r["id"] not in seen:    # O(N) per check
               seen.append(r["id"])
               out.append(r)
       return out

   def slowRanker(records):
       ranked = []
       for r in records:
           score = 0
           for keyword in ["urgent", "critical", "warning", "info", "debug"]:
               if keyword in r["message"]:    # O(len(message)) each
                   score += 1
           ranked.append({**r, "score": score})
       return ranked

   def slowSummary(records):
       summary = {}
       for r in records:
           key = r["category"]
           if key not in summary:    # building string key each time
               summary[key] = []
           summary[key].append(r["score"])
       means = {k: sum(v) / len(v) for k, v in summary.items()}
       return means

   Generate 5000 records: {"id": random int from 0–3000, "message": one of 5 keyword strings,
   "category": one of 10 category strings, "score": 0}.

2. Profile the full pipeline (deduplicator → ranker → summary) using cProfile.
   Print the top 10 functions by cumulative time. Identify the bottleneck.

3. Write optimized versions:
   - `fastDeduplicator`: use `set` for `seen`
   - `fastRanker`: precompile a set of keywords; use `any(kw in message for kw in keywords)`
   - `fastSummary`: use `collections.defaultdict(list)` to avoid the key existence check

4. Profile the optimized pipeline and print the top 10 functions. Compare cumulative
   times of the bottleneck functions before and after optimization.

5. Run both pipelines 50 times with timeit and print: slow pipeline mean time (ms),
   fast pipeline mean time (ms), speedup ratio. Verify that the speedup is consistent
   with what the profiler showed about the bottleneck.

No solutions provided.

===============================================================
EXERCISE 3 — Latency vs Throughput Tradeoff Analysis
===============================================================

Build a batched processing system and measure the latency/throughput tradeoff
across different batch sizes.

Requirements:

1. Write a function `processRecord(record)` that does: compute the sum of squares
   of all values in record["data"] (a list of 20 random floats), add the mean as
   a new field, return the augmented record. Add `time.sleep(0.001)` to simulate
   I/O overhead (e.g. writing to a database).

2. Write `processBatch(batch)` that calls processRecord on each record in the batch
   and returns the list of processed records.

3. Write `measureLatencyThroughput(batchSize, totalRecords)` that:
   - Generates `totalRecords` records with {"id": i, "data": [random floats × 20]}
   - Splits them into batches of `batchSize`
   - Measures: (a) latency of processing the FIRST record (time from start until
     the first record is fully processed, including waiting for the first batch to
     fill), and (b) total throughput in records/second
   - Returns {"batchSize": batchSize, "firstRecordLatencyMs": ..., "throughputRps": ...}

4. Run measureLatencyThroughput for batchSizes = [1, 5, 10, 50, 100, 500] with
   totalRecords = 1000. Print a table of batchSize, firstRecordLatencyMs, throughputRps.

5. Plot (as a text chart or print) the tradeoff: as batchSize increases, latency
   increases and throughput increases (up to a point). Identify the batch size that
   achieves at least 90% of maximum throughput while keeping first-record latency
   below 50ms. This is the "sweet spot" for systems with both latency and throughput
   requirements.

No solutions provided.

===============================================================
EXERCISE 4 — Python Performance Playbook in Practice
===============================================================

Implement a string processing pipeline and apply Python performance techniques
systematically, measuring the improvement of each technique.

Requirements:

1. You are given a list of 10,000 log lines in the format:
   "2024-01-15 10:23:45 ERROR sensorId=S42 value=78.3 message=threshold exceeded"
   Generate them with random timestamps, levels (DEBUG/INFO/WARNING/ERROR/CRITICAL),
   sensor IDs (S0–S99), values, and messages from a fixed vocabulary of 10 options.

2. Write a "naive" pipeline that:
   a. Filters for ERROR and CRITICAL lines using `line.startswith(level)` (must scan from start)
   b. Extracts sensorId by splitting on spaces then searching with a for loop
   c. Builds a report string by concatenating in a loop: `report += f"Sensor {sid}: ..."+ "\n"`
   d. Finds unique sensor IDs by appending to a list and checking membership
   Measure with timeit (100 repetitions).

3. Write an "optimized" pipeline that:
   a. Filters using a pre-built set {"ERROR", "CRITICAL"} and `line.split()[2] in levelSet`
   b. Extracts sensorId using `line.split(" sensorId=")[1].split()[0]` (single split)
   c. Builds the report using a list of lines joined at the end
   d. Collects unique sensor IDs into a set from the start
   Measure with timeit (100 repetitions).

4. For each optimization (a, b, c, d) independently, measure the speedup versus the naive
   version of that step. Print: "Step X: naive=Xms, optimized=Xms, speedup=Xx".

5. Compare total pipeline time: naive vs. optimized. Print the overall speedup.
   Identify which step had the largest impact — does this match your expectation
   before measuring, or did the profiler reveal a surprise?

No solutions provided.
