# Key Points — Lecture 31: Concurrency Architecture Patterns

---

- **The three-layer concurrency model assigns each workload type to the mechanism that handles it correctly: asyncio for I/O-bound work with async-compatible libraries, ThreadPoolExecutor for I/O-bound work with synchronous blocking libraries, and ProcessPoolExecutor for CPU-bound work requiring true parallelism.** Each mechanism solves one specific problem. Asyncio handles hundreds of concurrent I/O operations on a single thread by suspending coroutines at await points and resuming them when the operation completes — no thread overhead, no GIL pressure. ThreadPoolExecutor moves blocking library calls off the event loop thread, preventing them from freezing asyncio while still handling I/O concurrently. ProcessPoolExecutor gives each worker its own Python interpreter and its own GIL, enabling true multi-core CPU parallelism that threading cannot provide. Using the wrong mechanism for a workload produces a predictable failure: async for CPU-bound work freezes the event loop; threads for CPU-bound work hit the GIL ceiling; processes for I/O work pay IPC serialization cost on every request.

- **The fan-out / fan-in pattern reduces latency for distributed multi-backend queries from the sum of backend latencies to the maximum of backend latencies, achieving N-fold speedup for N independent backends.** Fan-out dispatches one query simultaneously to all N backends using `asyncio.create_task` for each backend followed by `asyncio.gather` or `asyncio.wait` to collect results. Fan-in aggregates the N independent results into one unified answer. This pattern is the architectural core of distributed search engines (Elasticsearch, Solr), distributed databases (Cassandra, Bigtable), and microservice orchestrators (service mesh sidecars, API gateways). Without fan-out, sequential queries to 8 shards with 200ms latency each produce 1600ms total latency; with fan-out, the same 8 shards produce 200ms total latency plus a small coordination overhead.

- **Tail latency management through `asyncio.wait` with timeout prevents a single slow backend from dominating the response time of a fan-out query.** In any distributed system with many backends, the probability that at least one backend is slow, degraded, or paging at any given moment is non-negligible. Without a timeout, a single 2-second outlier makes every query take 2 seconds. With `asyncio.wait(tasks, timeout=deadline)`, the call returns when the deadline expires, partitioning tasks into `done` (completed within deadline) and `pending` (still running). Pending tasks are cancelled with `task.cancel()` and cleaned up with `asyncio.gather(*pending, return_exceptions=True)`. The partial result from the responding shards is returned immediately. Google's "Tail at Scale" paper (Dean and Barroso, 2013) formalizes this as the core technique for latency management in large-scale distributed systems.

- **The circuit breaker pattern prevents cascading failures by tracking downstream service failure rates and "opening the circuit" — failing fast without contacting the service — when failures exceed a threshold.** Without a circuit breaker, a slow or failed downstream service that times out after 5 seconds causes every caller to hold connection slots, coroutines, and memory for 5 seconds per request. With 100 simultaneous callers, the aggregate resource consumption is 500 seconds of blocked capacity — enough to exhaust the caller's thread pool or connection pool, causing the caller itself to fail. The circuit breaker eliminates this amplification: after the failure threshold is crossed, subsequent calls fail immediately (under 1 millisecond) without resource consumption. Three states — closed (normal), open (failing fast), and half-open (probing with one test request to check recovery) — provide controlled recovery without requiring human intervention. The pattern was named and popularized by Michael Nygard in "Release It!" (2007) and is standard infrastructure in any microservice architecture.

- **The architecture decision framework reduces the choice of concurrency mechanism to four questions: I/O-bound vs CPU-bound; async-compatible vs synchronous libraries; scale of concurrent operations; and anticipated failure modes.** I/O-bound work that uses async-compatible libraries goes to asyncio directly. I/O-bound work with synchronous blocking libraries goes to a ThreadPoolExecutor, bridged to asyncio with `loop.run_in_executor`. CPU-bound work goes to a ProcessPoolExecutor with one worker per CPU core. Scale drives resource sizing: thousands of simultaneous connections require async (threads cannot scale to thousands); a few dozen parallel CPU tasks require a process pool (async cannot parallelize CPU work). Failure mode analysis determines resilience requirements: async services need circuit breakers and deadline timeouts; process pools require picklable arguments and worker crash detection; thread pools require lock ordering discipline to prevent deadlock. Without this framework, developers default to the concurrency model they know best and apply it to workloads where it produces systematic performance regressions or correctness bugs.

- **The bridge between asyncio and executor-based pools is `loop.run_in_executor(executor, func, *args)`, which makes any synchronous function awaitable without blocking the event loop.** The function and its arguments are pickled (for process pools) or passed directly (for thread pools), executed in the executor, and the result is delivered back to the event loop as a resolved Future. From the coroutine's perspective, `await loop.run_in_executor(pool, func, args)` is indistinguishable from any other await — the event loop continues handling other coroutines while the executor runs. This bridge is what enables the three-tier architecture: async for routing and orchestration, thread pool for sync I/O, process pool for computation, all wired together through a single async handler that awaits each as needed.

