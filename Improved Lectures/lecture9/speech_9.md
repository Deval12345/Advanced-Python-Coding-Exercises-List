In the previous lecture we built custom iterators from scratch, implementing dunder iter and dunder next manually. We saw that this works, but it requires careful state management. You had to write a class, track a position variable, handle StopIteration by hand, and make sure dunder iter returned self while dunder next advanced the state correctly. Python offers a far simpler way to create iterators: generator functions. With a single keyword, yield, Python handles all the iterator machinery for you. No class required. No manual state tracking. No explicit StopIteration. Just a function that uses yield wherever it wants to produce a value.

A generator function is any function that contains a yield statement. The presence of yield changes everything about how Python treats the function. When you call a generator function, Python does not run the function body immediately. Instead, it creates a generator object and returns it. The function body only runs when you start iterating over the generator object, when you call next on it or use it in a for loop.

Each time the generator executes a yield statement, two things happen: it produces a value for the caller, and it pauses execution. The function's local variables, its position in the code, and everything about its current state is preserved exactly as it was at the moment of pausing. The generator is suspended mid-execution. When the consumer asks for the next value, the generator resumes exactly where it paused. It picks up from the line after the yield statement and continues running until it hits the next yield or reaches the end of the function. When the function body finishes, Python automatically raises StopIteration. The generator knows it is done, and it signals this to the caller without you writing any explicit error handling.

This pause-and-resume behavior is what makes generators powerful. The generator function's execution is interleaved with the caller's execution. They take turns: the generator produces a value, hands control back to the caller, the caller processes it, then asks for the next value, and control returns to the generator.

Consider a simple countdown generator. We define a function that receives a start value, loops while that value is greater than zero, yields the current value, then decrements. Calling it with ten produces a generator object. When we iterate over that object, we get ten, nine, eight, all the way down to one. Python automatically implements the full iterator protocol. The generator object has both dunder iter and dunder next already working. We get a fully functioning iterator from a function that looks almost like a plain loop.

Now consider what happens when you need to process a large amount of data. A list comprehension computes all values immediately and stores them all in memory at once. For a million numbers, that means a million integer objects sitting in RAM before you have processed a single one. A generator expression computes values one at a time, on demand. For a million numbers, it holds only the current value and enough state to compute the next one. The memory usage is constant regardless of how many values the generator will eventually produce. For twenty values, both approaches behave identically from the caller's perspective. For twenty million values, the list could use hundreds of megabytes, while the generator uses a fixed, tiny amount. This is the meaning of lazy evaluation. The values are not computed until they are needed.

Generator expressions look like list comprehensions but use parentheses instead of square brackets. A list comprehension eagerly produces all values and stores them. A generator expression produces a generator object, and no values are computed until you iterate. When you pass a generator expression directly to a function like sum or max, you do not need the outer parentheses because the function call's own parentheses serve double duty. The practical rule: if you need all values at once or need to iterate multiple times, use a list. If you are doing a single pass, use a generator. The syntax difference is small. The memory difference can be enormous.

Think about a transaction log with millions of records. We define a generator function that receives the log lines, and for each line, strips whitespace, parses the amount and currency, and yields a dictionary with those values. Whether the log contains ten records or ten million, this generator processes one at a time. It does not load the entire log into memory. The caller consumes values as fast as it can process them. Memory stays bounded at all times. This pattern maps directly to reading large files: open the file, iterate line by line using a generator, and process each record without ever holding the entire file in memory.

Generators also enable a clean separation between the producer of data and the consumer of data. The producer is the generator function. It knows how to produce values, from a file, from an API, from a database, from a computation. It yields each value and waits. The consumer is whatever iterates the generator. It knows how to process values, aggregate them, filter them, write them somewhere. Neither the producer nor the consumer knows about the other's implementation. This decoupling is the core of the producer-consumer pattern. You can swap the producer without touching the consumer. You can swap the consumer without touching the producer. You can insert new processing stages between them. Each stage is independently testable.

A generator pipeline takes this further. We define three functions. The first reads raw records from a data source and yields each one. The second receives those records and yields only the ones with a positive amount, acting as a filter. The third receives the filtered records and yields computed totals for a specific currency, acting as a transformation. We chain them by passing each generator's output as input to the next. Each stage processes one value at a time. No stage waits for the previous stage to finish completely. When the consumer asks for a value, the request propagates back through the pipeline until a value is ready. The pipeline runs with constant memory regardless of how large the source data is.

The generator pattern is not just a Python trick. It underlies major systems and frameworks. Django's QuerySet is lazy. When you write a filter on a QuerySet, no database query runs. The query executes only when you iterate over the QuerySet. This is exactly the generator principle: describe the computation first, execute it only when needed. Apache Spark processes massive datasets across clusters using the same lazy evaluation model. Each transformation builds a plan. The plan executes only when you call an action. PyTorch's DataLoader uses a generator-like iterator to load batches lazily during training, processing one batch at a time rather than loading the entire dataset. The key insight across all these systems: lazy evaluation means you describe what to compute before how and when. You build a computation plan. You execute it only when needed, and only as much as needed.

Generators are not always the right tool. They are sequential and one-pass. Once a value has been yielded and consumed, it is gone. You cannot go back. If you need to re-read earlier values, you must either store them yourself or restart the generator. Generators do not support indexing. You cannot ask a generator for the fifth value without consuming the first four. If you need random access, use a list. Generators do not know their own length until fully consumed. Use generators for sequential processing of large or potentially infinite data, for one-pass pipelines, for streaming data where values arrive over time, and for situations where memory efficiency matters. Use lists for random access, for data you will iterate multiple times, and for operations that require the entire collection like sorting.

Generators turn complex iterator state machines into simple functions with yield. Where the previous lecture's pattern required a full class with explicit state management, a generator function requires none of that. The iterator protocol is implemented automatically. They enable lazy evaluation, which keeps memory bounded even for enormous datasets. A log file with fifty million lines, a real-time market feed with millions of events per day, a machine learning dataset with millions of images, all of these can be processed one item at a time with a generator, using only constant memory. Generator pipelines decouple producers from consumers, enabling each stage to be developed and tested independently. In the next lecture, we will explore decorators, which use the closure and first-class function concepts from our earlier lectures to inject behavior into functions non-invasively.
