# Lecture 23: Futures and Executors — The Bridge Between Sync and Async
## speech_23.md — Instructor Spoken Narrative

---

Let me ask you something. You've submitted tasks to a ThreadPoolExecutor, you've called executor dot map, you've seen results come back. But have you stopped to think about what you actually got back from executor dot submit? You got a Future. And today we are going to understand what that object actually is — because once you understand it, a huge amount of Python concurrency clicks into place.

So — what is a Future? A Future is a proxy object for a result that doesn't exist yet. When you submit a task to an executor, you get back this proxy immediately. The task might not have started. It might be halfway done. You don't know and you don't care — not yet. You hold onto that Future object and continue doing other work. Later — exactly when you need the result — you ask the Future for it.

Here's why that matters. The old way of writing concurrent code was callbacks. You passed a function to another function and said: when this task finishes, call this function with the result. Sounds reasonable. But in real programs with dozens of concurrent operations, callbacks nest inside callbacks. Exception handling becomes impossible — you can't use try and except across callback boundaries. Stack traces become meaningless. The industry literally invented a name for this: callback hell.

Futures are the escape from callback hell. Instead of scattering your logic across functions, you get a single object that travels through your code like any other variable. Submit a hundred tasks, get back a hundred Futures, keep working, collect results when you need them. The logic stays sequential and readable.

Now — a Future has states. It starts as pending, the moment you submit. When a worker thread picks it up, it moves to running. When it finishes, it moves to done — which could mean it finished successfully, it stored an exception, or it was cancelled before it even started. You can check whether a Future is done without blocking, using the done method. You can ask for the result using the result method — which blocks until the task is done, then either returns the value or re-raises any exception that occurred inside the worker. That exception propagation is elegant: your concurrent error handling looks exactly like your sequential error handling.

Now let's talk about the other executor — ProcessPoolExecutor. You know from Lecture 20 that the GIL limits thread-based code for CPU-bound work. No matter how many threads you create, they share one GIL and take turns running Python bytecode. For I/O-bound work that's fine — threads spend most of their time waiting anyway. But for genuine computation — prime counting, image processing, scientific calculations — you need real parallelism across cores.

ProcessPoolExecutor gives you that. Each process gets its own Python interpreter, its own GIL, its own memory. They run simultaneously on separate cores with no contention. And here's the elegant part — the API is identical. Switching from ThreadPoolExecutor to ProcessPoolExecutor is literally one line change. Same executor dot submit, same executor dot map, same Future objects.

But there's a cost you must understand. Processes don't share memory. Everything going in and out must be serialized — pickled — sent through inter-process communication channels, and deserialized on the other side. That serialization overhead is real. The rule: use ProcessPoolExecutor when each task does substantial CPU work, roughly a hundred milliseconds or more. If your tasks are tiny, the serialization cost dominates and you can end up slower than sequential code. Profile before assuming parallelism helps.

There's also a critical guard you must always include. On Windows and macOS, Python spawns new processes by importing your script. Without the if-name-equals-main guard, each spawned process imports your script, tries to create its own pool, which spawns more processes, which import the script — and you get an infinite spawn that hangs or crashes your machine. Always wrap your executor code in that guard. Let's trace through what the executor does in Example 23.1 — four identical workloads, counted sequentially versus in parallel across four processes. Watch the speedup.

Now, Future callbacks. The done callback method lets you register a function that fires automatically when a Future completes — success or failure. The callback receives the Future as its argument. You check whether there's an exception and handle accordingly. The key insight: you register this before the task is done. You're saying — I don't know when this will finish, but when it does, run this function. You can keep doing other things in the meantime.

Here's another tool you need: as completed. When you use executor dot map, results come back in submission order — task zero first, task one second, and so on, regardless of which actually finishes first. If task zero takes ten seconds and task five takes a tenth of a second, you wait ten seconds before you see task five's result. As completed is different. You hand it a collection of Futures and it yields them to you in the order they finish — fastest first. For systems that need to act on results the moment they're ready, as completed is the right choice. Let's trace through Example 23.2, where tasks randomly fail — watch how callbacks and as completed interact when some tasks raise exceptions.

Now we come to the most architecturally important concept in this lecture: the relationship between concurrent dot futures dot Future and asyncio dot Future. These are two different types. One is for thread and process pools. The other lives inside the event loop and is awaitable — you can use the await keyword on it inside a coroutine.

The bridge between them is loop dot run in executor. When you call it from inside a coroutine, you hand it a callable and an executor, and it returns an asyncio Future. You await that Future. The event loop suspends your coroutine and goes off to run other tasks. Meanwhile, your blocking function runs in a thread. When the thread finishes, the event loop resumes your coroutine with the result. The event loop never blocked. Your blocking code ran safely in a thread. The two worlds coexist cleanly.

This is the hybrid pattern you will see in production systems everywhere. The event loop handles thousands of concurrent connections — it's extraordinary at that. But when a request needs to call a legacy synchronous library: a PDF renderer, an old image processing package, a database driver that isn't async-native — you run it in a thread pool via run in executor. The event loop stays responsive. In Example 23.3, we call a blocking function four times concurrently this way. All four complete in roughly two hundred milliseconds total, not eight hundred — because the event loop dispatches all four to threads simultaneously.

Here's the core lesson for today: a Future is not just a convenience. It's a fundamental shift in how you reason about concurrent computation. You decouple when work starts from when you need its result. You centralize exception handling. You enable non-blocking callbacks. You bridge threads, processes, and async into a single mental model.

ThreadPoolExecutor gives you concurrent dot futures dot Futures backed by threads — excellent for I/O-bound work. ProcessPoolExecutor gives you the same object backed by processes — true CPU parallelism, with pickling costs and the if-name-equals-main requirement. Loop dot run in executor gives you an asyncio Future — the way blocking code lives safely inside an async program.

Next lecture we go higher still: asyncio dot gather with exception policies, asyncio dot wait with completion conditions, and asyncio dot Semaphore for rate-limiting concurrency at scale. But today — the Future is your foundation. Understand it deeply and everything else builds on it cleanly.
