# Key Points — Lecture 23: Futures and Executors — The Bridge Between Sync and Async

---

- **A Future is a proxy object for a result that does not exist yet.** When you submit a task to an executor, you receive a Future immediately — before the task has started, before it has finished, before you even know whether it will succeed. The Future object travels through your code like any other variable. You continue doing other work. When you need the result, you ask the Future for it. This decouples when work starts from when you need its result — a fundamental shift in how concurrent code is organized.

- **The Future abstraction replaced callback-based concurrency, solving the problem of callback hell.** Callback-based code fragments sequential logic across many nested functions. Exception handling across callback boundaries is effectively impossible — try/except does not span asynchronous callback chains. Stack traces become meaningless. Futures restore sequential reasoning: submit tasks, keep working, collect results with ordinary control flow and ordinary exception handling. Every major programming language has converged on this pattern for exactly this reason.

- **A `concurrent.futures.Future` moves through four states: PENDING, RUNNING, FINISHED (success), or holding a stored exception.** The `future.result()` method blocks the calling thread until the task completes, then either returns the value or re-raises any exception that occurred inside the worker — making concurrent error handling syntactically identical to sequential error handling. `future.done()` is a non-blocking check for completion status. `future.cancel()` succeeds only if the task has not yet started.

- **`ProcessPoolExecutor` gives Python true CPU parallelism by bypassing the GIL entirely.** Each worker is a separate OS process with its own Python interpreter, its own GIL, and its own memory. Multiple cores run simultaneously with no contention. The API is identical to `ThreadPoolExecutor` — switching is a one-line change. The cost is inter-process communication: all arguments and return values must be serialized using pickle, transmitted through a pipe, and deserialized on the other side. Use `ProcessPoolExecutor` only when each task does substantial CPU work — roughly 100 milliseconds or more — so serialization overhead is negligible compared to the work done.

- **The `if __name__ == "__main__"` guard is mandatory when using `ProcessPoolExecutor`.** On macOS and Windows, spawning a worker process requires importing the main script in the new process. Without the guard, each spawned process re-executes the top-level code, which spawns more processes, which import the script again — an infinite spawn cycle that hangs or crashes the machine. This guard is not optional; it is the minimum safe pattern for any multiprocessing code.

- **`future.add_done_callback(fn)` registers a function that fires automatically when the Future completes — success or failure.** The callback receives the Future as its argument, allowing you to check for exceptions and handle results without blocking. Multiple callbacks can be chained onto a single Future. If the Future is already done when the callback is registered, it fires immediately. This enables reactive programming patterns where you can continue doing other work and react to completion without any polling or blocking.

- **`concurrent.futures.as_completed(futures)` yields Futures in the order they finish, not the order they were submitted.** `executor.map` returns results in submission order — if the first task is the slowest, you wait for it before seeing any other result. `as_completed` yields the fastest result first, letting you process results the moment they are ready. For latency-sensitive pipelines — web scrapers, API fan-out clients, distributed data processors — `as_completed` is the correct tool.

- **`asyncio.Future` is the async-native counterpart to `concurrent.futures.Future`.** It is awaitable — you can use `await` on it inside a coroutine. `loop.run_in_executor(executor, fn, *args)` is the key bridge: it submits a blocking function to a thread or process pool and returns an `asyncio.Future`. The coroutine suspends at the `await`, freeing the event loop to run other coroutines while the worker thread does its blocking work. When the thread finishes, the event loop resumes the coroutine with the result.

- **The hybrid architecture — async event loop plus `run_in_executor` thread pool — is the production standard for Python services that must combine high-concurrency I/O with synchronous library calls.** The event loop handles thousands of concurrent connections with minimal memory. When a request must call a synchronous library (a PDF renderer, a legacy database driver, an image processing package), `run_in_executor` offloads that call to a thread. The event loop stays responsive for all other requests. Neither world is compromised; the Future abstraction makes them interoperate cleanly.
