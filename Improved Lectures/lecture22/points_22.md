# Key Points — Lecture 22: The Async Event Loop — How Cooperative Concurrency Works Under the Hood

---

- **The event loop is a single-threaded scheduler that manages thousands of concurrent I/O operations without creating thousands of OS threads.** One megabyte of stack per thread means ten thousand connections would require ten gigabytes of RAM just for thread stacks. The event loop sidesteps this entirely: coroutines suspended on I/O consume only a tiny stack frame, not a full OS thread, making hundreds of thousands of concurrent connections achievable on a single machine.

- **The event loop maintains two internal data structures: a ready queue and an I/O registry.** The ready queue holds coroutines that have work to do right now. The I/O registry maps suspended coroutines to the file descriptors or timers they are waiting on. When the ready queue empties, the loop makes a single OS call — `select`, `epoll` on Linux, or `kqueue` on macOS — which sleeps until at least one registered I/O operation completes. The OS then returns a list of ready descriptors, the loop moves those coroutines into the ready queue, and the cycle repeats.

- **The event loop runs a strict three-step cycle: run all ready coroutines until they yield, poll the OS for completed I/O, schedule newly-ready coroutines back into the ready queue.** "Cooperative" means the event loop never forcibly interrupts a running coroutine. A coroutine runs uninterrupted from one `await` expression to the next. If a coroutine runs a computation with no `await` points, it monopolizes the event loop thread for its entire duration — nothing else runs.

- **A single blocking call inside any coroutine freezes the entire event loop — not just that one coroutine.** This is not a subtle performance degradation; it is a complete stall of all concurrent work. Every other coroutine, background task, and heartbeat is frozen for the duration. The bug does not crash the server — it causes mysterious slowdowns under load, making it difficult to diagnose because the symptom appears distant from the cause.

- **Two strategies exist for dealing with blocking code inside async programs.** First, replace blocking calls with async-native equivalents: `asyncio.sleep` instead of `time.sleep`, `aiohttp` or `httpx` instead of `requests`, `aiofiles` instead of synchronous file I/O. Second, use `loop.run_in_executor(executor, fn, *args)` to offload the blocking call to a separate thread or process pool. The coroutine suspends at the `await` while the thread does the blocking work, keeping the event loop completely free.

- **`asyncio.sleep(0)` yields control to the event loop for one cycle without any actual time delay.** It is the manual cooperation point for CPU-heavy loops that perform real computation without hitting natural I/O await points. Without it, a loop processing a million items blocks all other coroutines for its entire duration. The pattern — insert `await asyncio.sleep(0)` every N iterations — gives the event loop regular windows to run heartbeats, health checks, and other ready tasks. N should be chosen so each batch takes only a few milliseconds.

- **Between any two consecutive `await` points in the same coroutine, that coroutine has exclusive use of the event loop thread.** No other coroutine can interleave with its code during that window. This means in-memory state mutated between two `await` points is safe from race conditions by design — no locks needed for that code. The tradeoff: any blocking call in that window freezes everything, so cooperative discipline is not optional; it is mandatory for the model to work correctly.

- **`asyncio.run(coroutine)` is the only recommended entry point for async programs.** It creates a new event loop, runs the given coroutine to completion, and then closes the loop and releases all resources including pending tasks. Never create event loops manually or leave them unclosed. `asyncio.get_event_loop()` returns the currently running loop from inside an async context, but `asyncio.run()` at the top level is the correct pattern for clean lifecycle management.

- **Choosing between async and threads depends on which libraries you control and what concurrency you need.** Async delivers its scalability advantage only when the entire library stack is async-compatible. A single synchronous call inside an otherwise async system blocks the event loop for all coroutines. When synchronous libraries are unavoidable, `run_in_executor` combines the scalability of async request handling with the compatibility of threads for blocking calls — the hybrid pattern used in the majority of production Python services.
