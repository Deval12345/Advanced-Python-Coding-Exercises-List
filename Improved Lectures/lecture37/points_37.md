# Key Points — Lecture 37: Big Project Stage 4 — Measurable Pipeline (Profiling and Metrics)

---

- **Without profiling, performance optimization is directed by cognitive bias — developers optimize the code they understand or the code that looks complex, not the code that is actually slow.** Amdahl's Law states that the total speedup from improving one component is bounded by the fraction of total runtime that component occupies. Optimizing a component that consumes 5% of runtime can produce at most a 5.26% system speedup, regardless of how completely that component is optimized. Optimizing a component that consumes 70% of runtime and making it twice as fast produces a 40% system speedup. The `cProfile` deterministic profiler maps every function call to its time contribution, revealing the actual distribution of runtime. Sorting by cumulative time — the time in a function plus all functions it calls — identifies the most expensive call chains. The rule: never write optimization code without first running `cProfile` and reading the report.

- **`cProfile` instruments every Python function call using C-level hooks, recording call counts and both total (exclusive) and cumulative (inclusive) time for each function, enabling precise identification of runtime hotspots.** `cProfile.Profile().enable()` and `.disable()` bracket the code section to profile — keeping this section tight (excluding setup and teardown) prevents noise in the report. `pstats.Stats(profiler).sort_stats("cumulative").print_stats(N)` formats and prints the top N functions. The cumulative time column identifies expensive call chains; the total time column identifies intrinsically slow functions that do heavy work before delegating. For the sensor analytics pipeline, `computeFeatures` typically dominates because `sum(window)` is called for every record — O(windowSize × N) additions that could be O(N) with an incremental rolling sum. The profiler makes this invisible cost immediately visible.

- **The `@measuredStage` decorator provides continuous, production-safe per-stage metrics without modifying the stage's business logic, by wrapping the generator method and measuring elapsed time at each yield point.** The decorator factory creates one `StageMetrics` accumulator per pipeline run, counting records in and out (revealing the filter ratio), accumulating per-yield latencies, and reporting throughput in records per second when the generator exhausts. Using `@functools.wraps` on the inner wrapper copies the original method's `__name__`, `__qualname__`, and `__doc__` to the wrapper, making the instrumented method indistinguishable from the original in stack traces, debuggers, and documentation tools. The decorator is applied at class definition time — there is no per-record branching cost for deciding whether to measure.

- **The timing point of the `@measuredStage` decorator — measuring elapsed time between the `yield` and the resume — captures the downstream consumer's processing time, not the stage's own processing time, making it a pipeline bottleneck finder rather than a function profiler.** When a generator yields, it suspends and passes control to its consumer. Execution resumes only when the consumer requests the next record. The elapsed time between yield and resume is entirely determined by how long the consumer took to process the yielded record. If the filter stage is fast but the normalizer is slow, the filter's recorded "latency" will be high — because it spends most of its time suspended, waiting for the normalizer to finish. This asymmetry is intentional: `cProfile` finds intrinsic function bottlenecks; `@measuredStage` latency finds pipeline chokepoints where time accumulates in the chain.

- **`__slots__` reduces per-instance memory by 2–3× for frequently instantiated objects by replacing the per-instance `__dict__` hash table with a fixed-size C-level slot array, and enforces the attribute contract by raising `AttributeError` on undeclared attribute assignments.** A standard Python object instance carries a `__dict__` that uses 200–360 bytes even for a sparse dictionary of 4–5 attributes. A `__slots__` object with the same attributes uses 50–100 bytes — the difference scales linearly with the number of instances. For a metrics system that creates one accumulator per stage per pipeline run, across millions of runs, the memory savings compound. Additionally, `__slots__` prevents the common typo bug: `self.totlaTime = x` on a dict-based object silently creates a new attribute; on a slots object, it immediately raises `AttributeError`, catching the bug at the point of error.

- **`tracemalloc` provides Python's built-in memory profiling capability — capturing snapshot comparisons that attribute memory allocations to specific source lines — enabling quantitative measurement of infrastructure memory overhead.** `tracemalloc.start()` activates allocation tracing; `tracemalloc.take_snapshot()` captures the current state of all live allocations; `snapshot2.compare_to(snapshot1, "lineno")` returns `StatisticDiff` objects per source line showing net allocations between snapshots. Summing `size_diff` across all statistics gives total bytes allocated. This is the measurement tool for confirming that `__slots__` saves the expected memory, for detecting unexpected allocation growth in long-running pipelines, and for setting evidence-based memory budget limits for caching and buffering infrastructure that will be added in Stage 5.

