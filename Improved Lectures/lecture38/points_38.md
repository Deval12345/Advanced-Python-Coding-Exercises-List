# Key Points — Lecture 38: Big Project Stage 5 — Caching and Memory Discipline

---

- **Memoization with `functools.lru_cache` eliminates redundant computation for pure functions called repeatedly with the same arguments, transforming O(N calls × computation cost) to O(distinct argument values × computation cost).** For a calibration lookup called 50,000 times with 10 distinct sensor IDs, the memoized version executes the lookup function 10 times (one per distinct sensor) and serves the remaining 49,990 calls from the cache instantly. The `maxsize` parameter controls the Least Recently Used eviction policy — when the cache is full and a new key arrives, the least recently accessed entry is evicted. Arguments must be hashable (strings, integers, tuples — not lists or dicts). `cache_info()` provides observability: `hits`, `misses`, `currsize`, and `maxsize`. `cache_clear()` forces all entries to be evicted — use when the underlying data has changed and the cache must reflect fresh values.

- **TTL (Time-To-Live) caching extends memoization to semi-stable values that change on a known schedule, by associating an expiry timestamp with each cached entry.** `lru_cache` is appropriate for truly pure functions whose output never changes. For functions whose output is stable for minutes or hours — configuration values, calibration parameters, feature flags — a TTL cache provides aggressive caching within the stable window and automatic freshness after the TTL expires. Implementation: a dictionary of `(value, expiry_time)` pairs, where `expiry_time = time.monotonic() + ttl_seconds` at the time of writing. On access, compare the current monotonic time to the expiry; if expired, evict and recompute. Use `time.monotonic()` (not `time.time()`) to ensure correctness when system clocks are adjusted. The TTL is a system design parameter tuned to the update frequency of the source: seconds for rapidly-changing data, hours for stable calibrations.

- **Every unbounded data structure in a streaming pipeline that runs indefinitely is a time-bomb: a structure that accumulates without limit will eventually exhaust available memory, causing process crashes that are impossible to reproduce in short-duration tests.** An alert accumulator appending to a plain list grows proportionally with alert rate and run duration. In a 5-minute test, it may accumulate 1,000 entries and appear fine. In 24-hour production operation at the same rate, it accumulates 288,000 entries. At 200 bytes per entry, that is 57 MB — manageable — but at higher alert rates or on systems with tight memory budgets, the accumulation leads to OOM (Out Of Memory) kills. The diagnostic signature is memory that climbs linearly from deployment until crash, with no obvious leak in the code inspection because the accumulation is intentional — just not bounded.

- **`collections.deque(maxlen=N)` is Python's standard bounded-sequence primitive: when the deque is full and a new element is appended, the oldest element is automatically discarded, maintaining exactly N elements with O(1) append and O(1) popleft.** `deque(maxlen=500)` stores the most recent 500 elements — when the 501st is appended, the 1st is silently dropped. Memory usage is constant after the deque is full, regardless of how many total elements were appended. Separate counters (like `_totalTriggered`) can track the total historical count without storing all historical data. The deque is the correct container for sliding windows, recent-history buffers, and any "keep the last N" use case.

- **The design discipline for production streaming systems requires that every accumulator, buffer, cache, and queue declares its maximum size and specifies the eviction behavior when that size is reached.** The appropriate bounded container depends on the desired eviction behavior: `deque(maxlen=N)` drops the oldest entry (preserves recent history); `lru_cache(maxsize=N)` evicts the least recently used entry (preserves popular entries); `TTLCache(maxSize=N)` evicts the soonest-to-expire entry (preserves longest-lived entries); `queue.Queue(maxsize=N)` applies backpressure by blocking producers (no data loss, but producers may wait); `asyncio.Queue(maxsize=N)` applies async backpressure by suspending producing coroutines. Each is a different trade-off between data loss, producer slowdown, and memory bounds. The choice must be explicit in system design — the default (no bound) is never correct for a long-running system.

- **`tracemalloc` snapshot comparison verifies memory discipline quantitatively: by comparing allocation snapshots before and after a pipeline run, you can confirm that bounded containers hold their declared limits and that memory growth is controlled.** `tracemalloc.start()` activates allocation tracing; `tracemalloc.take_snapshot()` captures the current allocation state; `snapshot2.compare_to(snapshot1, "lineno")` returns `StatisticDiff` objects showing net allocation per source line. For a correctly bounded accumulator, the total net allocation after N records should be approximately constant — the bound is respected. For an unbounded accumulator, the net allocation grows linearly with N, immediately visible in the snapshot comparison. This quantitative verification closes the measure-build-verify loop that Stage 4 established.

