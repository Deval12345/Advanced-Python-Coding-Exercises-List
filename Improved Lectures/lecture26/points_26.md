# Key Points — Lecture 26: Multiprocessing Part 2 — IPC Cost, Shared Memory, and When Parallelism Fails

---

- **Every object crossing a process boundary is serialized, transferred, and deserialized — three full data copies.** Pickling the object creates a bytes buffer in the sender (copy one). Writing those bytes through the OS kernel pipe copies them to the pipe buffer (copy two). The receiver reconstructs the object from the bytes (copy three). A 100-megabyte NumPy array crossing a process boundary generates 300 megabytes of peak memory pressure and 1 to 2 seconds of serialization time — before any computation begins. This is not an optimization concern; it is the fundamental constraint that determines whether multiprocessing helps or hurts.

- **The minimal IPC principle is an architectural requirement, not a coding style.** Design multiprocessing systems so that inter-process messages carry parameterization information, not data payloads. Send file paths, not file contents. Send array index ranges, not array slices. Send configuration parameters, not datasets. The worker reads from disk or shared memory directly, within its own process, without any cross-boundary transfer. This keeps IPC message size constant regardless of how large the input data grows.

- **`concurrent.futures.ProcessPoolExecutor` provides the same interface as `ThreadPoolExecutor` backed by processes — one-line switch for true CPU parallelism.** It returns genuine `concurrent.futures.Future` objects that compose with `as_completed`, `add_done_callback`, and `asyncio.wrap_future`. This is the preferred choice when process-based parallelism must compose with the futures ecosystem or with async code. `multiprocessing.Pool` offers finer-grained control — `maxtasksperchild` for worker memory leak mitigation, `pool.imap` for true streaming, daemon workers — making it better suited for complex worker lifecycle management.

- **The first failure scenario: tasks shorter than IPC round-trip overhead.** A warm process pool has no startup cost, but each task submission and result collection still costs 2 to 20 milliseconds of IPC overhead depending on payload size. If the computation takes 1 millisecond, the IPC overhead is 5 to 20 times the computation time. Eight 1-millisecond tasks take 8 milliseconds sequentially and 48 milliseconds in a pool with 5 milliseconds IPC overhead per task. Sequential wins by a factor of six. The solution is to batch small tasks into larger chunks before sending to workers, increasing per-worker computation time relative to the fixed IPC overhead.

- **The second failure scenario: large data payload that dominates IPC cost.** When pickling the task input takes longer than the computation itself, parallelism cannot compensate. A task that processes a 100-megabyte array in 200 milliseconds but requires 2 seconds to serialize sees 90% of its wall-clock time in serialization. Four workers each serialize independently — four times the memory pressure and four times the serialization overhead — while the computation runs in parallel. The aggregate effect is marginal speedup at the cost of massive memory pressure. The solution is shared memory, where workers read from the same physical memory without any serialization at all.

- **The third failure scenario: frequent inter-worker coordination through Manager objects.** Every call to a Manager-hosted shared object crosses a process boundary. Workers that check a shared flag ten times per second generate ten IPC round-trips per second — multiplied by the number of workers. With four workers, that is forty IPC calls per second purely for coordination overhead. Tasks that require constant coordination may not scale at all with worker count, because increasing workers increases coordination traffic proportionally. The correct design for cooperative tasks is to partition work so each worker operates independently on its own data slice.

- **The crossover point — the minimum task duration at which multiprocessing reliably helps — typically falls between 10 and 50 milliseconds on modern hardware.** Below this threshold, IPC overhead dominates. Above 200 milliseconds with a small output payload, multiprocessing almost always wins. In the range between these values, measure before committing. The crossover varies with data payload size, system memory bandwidth, number of cores, and Python environment size (which determines process startup time and baseline memory usage).

- **Profile before committing to any multiprocessing architecture.** Write the sequential version first and measure it. Create an isolated benchmark with the same task, the same data, and the same workload running sequentially versus in a Pool of N workers. If the speedup is less than 1.2× in the benchmark, question the architecture — it will not be faster in production. If batching tasks to increase per-worker compute time does not help, examine whether the bottleneck is IPC, memory bandwidth, or whether the work is genuinely CPU-bound in the first place. Assumption-driven architectural decisions are the primary source of multiprocessing performance regressions.
