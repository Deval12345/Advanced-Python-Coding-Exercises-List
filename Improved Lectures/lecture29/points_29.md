# Key Points — Lecture 29: Async + Multiprocessing Hybrid Architecture

---

- **The hybrid architecture exists because I/O concurrency and CPU parallelism are fundamentally different problems that require different solutions.** Async handles I/O concurrency efficiently: a single thread in an event loop multiplexes thousands of simultaneous clients because each client spends most of its time waiting for network bytes, database results, or filesystem access — not executing Python. Multiprocessing handles CPU parallelism: separate processes on separate cores bypass the GIL and execute Python computation simultaneously. Neither paradigm handles the other's workload well. Async running CPU-intensive code freezes the event loop; a pure process pool has no mechanism for I/O multiplexing across thousands of connections. The hybrid uses each for its intended purpose.

- **Running CPU-bound work synchronously inside a coroutine freezes the event loop for all other clients for the duration of that computation.** When a coroutine executes Python CPU code without yielding — numpy operations, mathematical computation, model inference — the event loop cannot switch to other coroutines. A 200-millisecond inference running in the event loop pauses every other client's processing for 200 milliseconds. With 100 simultaneous clients, the effective throughput collapses to 5 requests per second regardless of how many concurrent connections the system accepts. This is the fundamental failure mode that the hybrid architecture is designed to prevent.

- **loop.run_in_executor is the correct bridge from a coroutine to a ProcessPoolExecutor: it submits the work, suspends the coroutine, lets the event loop continue, and resumes the coroutine with the result when the worker finishes.** The call `await loop.run_in_executor(executor, fn, arg)` submits fn(arg) to the executor and immediately suspends the coroutine. The event loop continues handling other coroutines while the process pool worker executes the function. When the worker finishes, the event loop is notified and resumes the awaiting coroutine with the return value. From the coroutine's perspective, it awaited a result. From the event loop's perspective, the coroutine was suspended and other work ran. From the process pool's perspective, it received a regular function call.

- **The ProcessPoolExecutor must be created once at application startup and shared across all request handlers — never created per request.** Process pool startup costs 50 to 200 milliseconds per worker because each worker is a fresh OS process with its own Python interpreter that must import all required modules. Creating a new executor per request adds this startup cost to every single request's latency. The correct pattern is to create the executor in the application's main coroutine or startup hook and pass it as an argument to all handler coroutines. In web frameworks, this means creating the executor in a lifespan event and closing it in the corresponding shutdown event.

- **asyncio.Queue with a bounded maxsize provides backpressure in producer-consumer pipelines, preventing the producer from getting too far ahead of the consumer and exhausting memory.** When a fast async producer generates jobs faster than the process pool can consume them, an unbounded queue accumulates all pending jobs in memory. For jobs carrying large input data — arrays, documents, images — this can exhaust available RAM before workers have processed a single batch. Setting maxsize=N on the asyncio.Queue causes the producer to block at await jobQueue.put when the queue holds N items, effectively throttling the producer to the consumer's rate. The maxsize parameter is the backpressure valve that makes streaming pipelines memory-safe.

- **Only picklable synchronous functions can be submitted to a ProcessPoolExecutor — coroutines cannot cross the process boundary.** When run_in_executor submits work to a process pool, the function and its arguments must be picklable because they are serialized and sent to a worker process. Coroutines are generator-based Python objects that cannot be pickled. Attempting to submit a coroutine to executor.submit raises a confusing error or executes incorrectly. The architectural boundary is absolute: everything above run_in_executor is async code in the main process; everything below it is synchronous Python functions in worker processes. This division must be designed deliberately, not discovered through runtime errors.

- **The event loop is process-local: each process has its own isolated asyncio event loop and coroutines cannot be shared or transferred between processes.** Calling asyncio.run() in a worker process creates a completely new event loop with no connection to the main process's event loop. There is no shared event loop state. There is no mechanism to send a coroutine to a worker process for execution. Event loop objects cannot be pickled. Developers who attempt to share an event loop across processes encounter pickling errors or undefined behavior. The correct architecture keeps all async code in the main process and uses synchronous functions in all worker processes.

- **The producer-consumer hybrid pattern combines asyncio.gather for concurrent coroutine execution with run_in_executor for process pool dispatch, enabling a streaming pipeline that is simultaneously I/O-concurrent and CPU-parallel.** The producer coroutine generates jobs asynchronously with I/O delays, putting them into an asyncio.Queue. The submitter coroutine reads from the queue and dispatches each job to the process pool via run_in_executor, collecting the resulting awaitables. asyncio.gather runs both coroutines concurrently in the event loop. After the queue is drained, asyncio.gather on the collected futures waits for all process pool results. This pattern handles streaming data arriving at variable I/O rates while maintaining full CPU core utilization for the compute-intensive processing of each item.
